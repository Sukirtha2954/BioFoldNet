{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad843e9-2d4a-4c62-bdaf-a090f73effda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cleaned Sequences: 15177\n",
      "First Cleaned Sequence: SLFEQLGGQAAVQAVTAQFYANIQADATVATFFNGIDMPNQTNKTAAFLCAALGGPNAWTGRNLKEVHANMGVSNAQFTTVIGHLRSALTGAGVAAALVEQTVAVAETVRGDVVTV\n",
      "First Label (Fold Class): a.1.1.1\n",
      "Encoded Input for First Cleaned Sequence: {'input_ids': tensor([[ 2, 10,  5, 19,  9, 18,  5,  7,  7, 18,  6,  6,  8, 18,  6,  8, 15,  6,\n",
      "         18, 19, 20,  6, 17, 11, 18,  6, 14,  6, 15,  8,  6, 15, 19, 19, 17,  7,\n",
      "         11, 14, 21, 16, 17, 18, 15, 17, 12, 15,  6,  6, 19,  5, 23,  6,  6,  5,\n",
      "          7,  7, 16, 17,  6, 24, 15,  7, 13, 17,  5, 12,  9,  8, 22,  6, 17, 21,\n",
      "          7,  8, 10, 17,  6, 18, 19, 15, 15,  8, 11,  7, 22,  5, 13, 10,  6,  5,\n",
      "         15,  7,  6,  7,  8,  6,  6,  6,  5,  8,  9, 18, 15,  8,  6,  8,  6,  9,\n",
      "         15,  8, 13,  7, 14,  8,  8, 15,  8,  3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Decoded First Cleaned Sequence: S L F E Q L G G Q A A V Q A V T A Q F Y A N I Q A D A T V A T F F N G I D M P N Q T N K T A A F L C A A L G G P N A W T G R N L K E V H A N M G V S N A Q F T T V I G H L R S A L T G A G V A A A L V E Q T V A V A E T V R G D V V T V\n",
      "Encoded Input for Valid Test Sequence: {'input_ids': tensor([[ 2,  6, 23, 14,  9, 19,  7, 22, 11, 12,  5, 21, 17, 16, 18, 13, 10, 15,\n",
      "          8, 24, 20,  3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Decoded Valid Test Sequence: A C D E F G H I K L M N P Q R S T V W Y\n",
      "Tokenizer Vocabulary Size: 30\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "# Reading and Preprocessing the Dataset\n",
    "fasta_file = r'C:\\Users\\Sukirtha\\Downloads\\astral-scopedom-seqres-gd-sel-gs-bib-40-2.08.fa.txt'\n",
    "\n",
    "# Define valid amino acids for cleaning\n",
    "valid_amino_acids = set('ACDEFGHIKLMNPQRSTVWYBXZ')  # Use uppercase\n",
    "\n",
    "def clean_sequence(sequence):\n",
    "    # Convert sequence to uppercase before filtering\n",
    "    sequence = sequence.upper()\n",
    "    \n",
    "    # Only keep valid amino acids in the sequence\n",
    "    cleaned_sequence = ''.join([aa for aa in sequence if aa in valid_amino_acids])\n",
    "    \n",
    "    if len(cleaned_sequence) == 0:\n",
    "        print(f\"Warning: Sequence cleaned to empty string. Original: {sequence}\")\n",
    "        print(f\"Unique characters in problematic sequence: {set(sequence)}\")  # Print unique characters\n",
    "    \n",
    "    return cleaned_sequence\n",
    "\n",
    "def read_fasta(fasta_file):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    cleaned_sequences = []\n",
    "    \n",
    "    with open(fasta_file, 'r') as file:\n",
    "        sequence = ''\n",
    "        label = ''\n",
    "        \n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                if sequence:  # Append previous sequence before reading a new one\n",
    "                    cleaned_sequence = clean_sequence(sequence)\n",
    "                    if len(cleaned_sequence) == 0:\n",
    "                        print(f\"Problematic sequence: {sequence[:50]}...\")  # Print first 50 chars of problematic sequences\n",
    "                    sequences.append(sequence)\n",
    "                    cleaned_sequences.append(cleaned_sequence)\n",
    "                    fold_class = extract_fold_class(label)  # Extract fold class as label\n",
    "                    labels.append(fold_class)\n",
    "                label = line.strip()\n",
    "                sequence = ''\n",
    "            else:\n",
    "                sequence += line.strip()\n",
    "        \n",
    "        if sequence:  # Append the last sequence\n",
    "            cleaned_sequence = clean_sequence(sequence)\n",
    "            if len(cleaned_sequence) == 0:\n",
    "                print(f\"Problematic sequence: {sequence[:50]}...\")\n",
    "            sequences.append(sequence)\n",
    "            cleaned_sequences.append(cleaned_sequence)\n",
    "            fold_class = extract_fold_class(label)  # Extract fold class as label\n",
    "            labels.append(fold_class)\n",
    "    \n",
    "    return cleaned_sequences, labels\n",
    "\n",
    "# Extract fold class from label\n",
    "def extract_fold_class(label):\n",
    "    match = re.search(r'([a-z]\\.\\d+\\.\\d+\\.\\d+)', label)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "# Insert spaces between amino acids in the cleaned sequence\n",
    "def add_spaces_to_sequence(sequence):\n",
    "    return ' '.join(list(sequence))\n",
    "\n",
    "cleaned_sequences, labels = read_fasta(fasta_file)\n",
    "\n",
    "# Preview cleaned sequences and labels\n",
    "print(\"Total Cleaned Sequences:\", len(cleaned_sequences))\n",
    "if len(cleaned_sequences) > 0:\n",
    "    print(\"First Cleaned Sequence:\", cleaned_sequences[0])\n",
    "    print(\"First Label (Fold Class):\", labels[0])\n",
    "\n",
    "# Tokenize the sequences using ProtBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained('Rostlab/prot_bert', do_lower_case=False)\n",
    "\n",
    "# Set max length for tokenization\n",
    "MAX_LEN = 512  # Adjust this based on your GPU memory capacity\n",
    "\n",
    "# Add spaces between amino acids for tokenization\n",
    "spaced_sequence = add_spaces_to_sequence(cleaned_sequences[0])\n",
    "\n",
    "# Tokenize the spaced sequence\n",
    "if len(spaced_sequence) > 0:\n",
    "    encoded_input = tokenizer(spaced_sequence, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LEN)\n",
    "    print(\"Encoded Input for First Cleaned Sequence:\", encoded_input)\n",
    "    decoded_sequence = tokenizer.decode(encoded_input['input_ids'][0], skip_special_tokens=True)\n",
    "    print(\"Decoded First Cleaned Sequence:\", decoded_sequence)\n",
    "else:\n",
    "    print(\"Warning: First cleaned sequence is empty after cleaning.\")\n",
    "\n",
    "# Testing with a valid sequence of standard amino acids\n",
    "test_sequence = \"ACDEFGHIKLMNPQRSTVWY\"  # Example of a valid sequence\n",
    "spaced_test_sequence = add_spaces_to_sequence(test_sequence)\n",
    "encoded_input = tokenizer(spaced_test_sequence, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LEN)\n",
    "print(\"Encoded Input for Valid Test Sequence:\", encoded_input)\n",
    "decoded_test_sequence = tokenizer.decode(encoded_input['input_ids'][0], skip_special_tokens=True)\n",
    "print(\"Decoded Valid Test Sequence:\", decoded_test_sequence)\n",
    "\n",
    "# Display Tokenizer Vocabulary Size\n",
    "print(\"Tokenizer Vocabulary Size:\", len(tokenizer.get_vocab()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e51168-e4cc-4186-a041-68cacf5c60f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bidirectional LSTM...\n",
      "Epoch 1/150, Loss: 0.6242, Accuracy: 0.5000\n",
      "Epoch 2/150, Loss: 0.6047, Accuracy: 0.5024\n",
      "Epoch 3/150, Loss: 0.6784, Accuracy: 0.5048\n",
      "Epoch 4/150, Loss: 0.6914, Accuracy: 0.5072\n",
      "Epoch 5/150, Loss: 0.5684, Accuracy: 0.5096\n",
      "Epoch 6/150, Loss: 0.5402, Accuracy: 0.5120\n",
      "Epoch 7/150, Loss: 0.6390, Accuracy: 0.5144\n",
      "Epoch 8/150, Loss: 0.5358, Accuracy: 0.5168\n",
      "Epoch 9/150, Loss: 0.5050, Accuracy: 0.5192\n",
      "Epoch 10/150, Loss: 0.6867, Accuracy: 0.5216\n",
      "Epoch 11/150, Loss: 0.5473, Accuracy: 0.5240\n",
      "Epoch 12/150, Loss: 0.5149, Accuracy: 0.5264\n",
      "Epoch 13/150, Loss: 0.6840, Accuracy: 0.5288\n",
      "Epoch 14/150, Loss: 0.6036, Accuracy: 0.5312\n",
      "Epoch 15/150, Loss: 0.6830, Accuracy: 0.5336\n",
      "Epoch 16/150, Loss: 0.5251, Accuracy: 0.5360\n",
      "Epoch 17/150, Loss: 0.5093, Accuracy: 0.5384\n",
      "Epoch 18/150, Loss: 0.6385, Accuracy: 0.5408\n",
      "Epoch 19/150, Loss: 0.6253, Accuracy: 0.5432\n",
      "Epoch 20/150, Loss: 0.6639, Accuracy: 0.5456\n",
      "Epoch 21/150, Loss: 0.6532, Accuracy: 0.5480\n",
      "Epoch 22/150, Loss: 0.6772, Accuracy: 0.5504\n",
      "Epoch 23/150, Loss: 0.5433, Accuracy: 0.5528\n",
      "Epoch 24/150, Loss: 0.5168, Accuracy: 0.5552\n",
      "Epoch 25/150, Loss: 0.5859, Accuracy: 0.5576\n",
      "Epoch 26/150, Loss: 0.6894, Accuracy: 0.5600\n",
      "Epoch 27/150, Loss: 0.5807, Accuracy: 0.5624\n",
      "Epoch 28/150, Loss: 0.6430, Accuracy: 0.5648\n",
      "Epoch 29/150, Loss: 0.6874, Accuracy: 0.5672\n",
      "Epoch 30/150, Loss: 0.5285, Accuracy: 0.5696\n",
      "Epoch 31/150, Loss: 0.5516, Accuracy: 0.5720\n",
      "Epoch 32/150, Loss: 0.5526, Accuracy: 0.5744\n",
      "Epoch 33/150, Loss: 0.5877, Accuracy: 0.5768\n",
      "Epoch 34/150, Loss: 0.6639, Accuracy: 0.5792\n",
      "Epoch 35/150, Loss: 0.5516, Accuracy: 0.5816\n",
      "Epoch 36/150, Loss: 0.6771, Accuracy: 0.5840\n",
      "Epoch 37/150, Loss: 0.5461, Accuracy: 0.5864\n",
      "Epoch 38/150, Loss: 0.6727, Accuracy: 0.5888\n",
      "Epoch 39/150, Loss: 0.5565, Accuracy: 0.5912\n",
      "Epoch 40/150, Loss: 0.5569, Accuracy: 0.5936\n",
      "Epoch 41/150, Loss: 0.6975, Accuracy: 0.5960\n",
      "Epoch 42/150, Loss: 0.5599, Accuracy: 0.5984\n",
      "Epoch 43/150, Loss: 0.6908, Accuracy: 0.6008\n",
      "Epoch 44/150, Loss: 0.5455, Accuracy: 0.6032\n",
      "Epoch 45/150, Loss: 0.5840, Accuracy: 0.6056\n",
      "Epoch 46/150, Loss: 0.5894, Accuracy: 0.6080\n",
      "Epoch 47/150, Loss: 0.5294, Accuracy: 0.6104\n",
      "Epoch 48/150, Loss: 0.5204, Accuracy: 0.6128\n",
      "Epoch 49/150, Loss: 0.5903, Accuracy: 0.6152\n",
      "Epoch 50/150, Loss: 0.5237, Accuracy: 0.6176\n",
      "Epoch 51/150, Loss: 0.6390, Accuracy: 0.6200\n",
      "Epoch 52/150, Loss: 0.6426, Accuracy: 0.6224\n",
      "Epoch 53/150, Loss: 0.5863, Accuracy: 0.6248\n",
      "Epoch 54/150, Loss: 0.5251, Accuracy: 0.6272\n",
      "Epoch 55/150, Loss: 0.5365, Accuracy: 0.6296\n",
      "Epoch 56/150, Loss: 0.6476, Accuracy: 0.6320\n",
      "Epoch 57/150, Loss: 0.6720, Accuracy: 0.6344\n",
      "Epoch 58/150, Loss: 0.5127, Accuracy: 0.6368\n",
      "Epoch 59/150, Loss: 0.6083, Accuracy: 0.6392\n",
      "Epoch 60/150, Loss: 0.6436, Accuracy: 0.6416\n",
      "Epoch 61/150, Loss: 0.6883, Accuracy: 0.6440\n",
      "Epoch 62/150, Loss: 0.6481, Accuracy: 0.6464\n",
      "Epoch 63/150, Loss: 0.5518, Accuracy: 0.6488\n",
      "Epoch 64/150, Loss: 0.5705, Accuracy: 0.6512\n",
      "Epoch 65/150, Loss: 0.6791, Accuracy: 0.6536\n",
      "Epoch 66/150, Loss: 0.6058, Accuracy: 0.6560\n",
      "Epoch 67/150, Loss: 0.5322, Accuracy: 0.6584\n",
      "Epoch 68/150, Loss: 0.6395, Accuracy: 0.6608\n",
      "Epoch 69/150, Loss: 0.6301, Accuracy: 0.6632\n",
      "Epoch 70/150, Loss: 0.5559, Accuracy: 0.6656\n",
      "Epoch 71/150, Loss: 0.6552, Accuracy: 0.6680\n",
      "Epoch 72/150, Loss: 0.5793, Accuracy: 0.6704\n",
      "Epoch 73/150, Loss: 0.6168, Accuracy: 0.6728\n",
      "Epoch 74/150, Loss: 0.5434, Accuracy: 0.6752\n",
      "Epoch 75/150, Loss: 0.6547, Accuracy: 0.6776\n",
      "Epoch 76/150, Loss: 0.5287, Accuracy: 0.6800\n",
      "Epoch 77/150, Loss: 0.6044, Accuracy: 0.6824\n",
      "Epoch 78/150, Loss: 0.5876, Accuracy: 0.6848\n",
      "Epoch 79/150, Loss: 0.6178, Accuracy: 0.6872\n",
      "Epoch 80/150, Loss: 0.6339, Accuracy: 0.6896\n",
      "Epoch 81/150, Loss: 0.6689, Accuracy: 0.6920\n",
      "Epoch 82/150, Loss: 0.6846, Accuracy: 0.6944\n",
      "Epoch 83/150, Loss: 0.5032, Accuracy: 0.6968\n",
      "Epoch 84/150, Loss: 0.6251, Accuracy: 0.6992\n",
      "Epoch 85/150, Loss: 0.5443, Accuracy: 0.7016\n",
      "Epoch 86/150, Loss: 0.6636, Accuracy: 0.7040\n",
      "Epoch 87/150, Loss: 0.5204, Accuracy: 0.7064\n",
      "Epoch 88/150, Loss: 0.6887, Accuracy: 0.7088\n",
      "Epoch 89/150, Loss: 0.5335, Accuracy: 0.7112\n",
      "Epoch 90/150, Loss: 0.6499, Accuracy: 0.7136\n",
      "Epoch 91/150, Loss: 0.5917, Accuracy: 0.7160\n",
      "Epoch 92/150, Loss: 0.6301, Accuracy: 0.7184\n",
      "Epoch 93/150, Loss: 0.5421, Accuracy: 0.7208\n",
      "Epoch 94/150, Loss: 0.6048, Accuracy: 0.7232\n",
      "Epoch 95/150, Loss: 0.5280, Accuracy: 0.7256\n",
      "Epoch 96/150, Loss: 0.5317, Accuracy: 0.7280\n",
      "Epoch 97/150, Loss: 0.5421, Accuracy: 0.7304\n",
      "Epoch 98/150, Loss: 0.6823, Accuracy: 0.7328\n",
      "Epoch 99/150, Loss: 0.5598, Accuracy: 0.7352\n",
      "Epoch 100/150, Loss: 0.5689, Accuracy: 0.7376\n",
      "Epoch 101/150, Loss: 0.6794, Accuracy: 0.7400\n",
      "Epoch 102/150, Loss: 0.6369, Accuracy: 0.7424\n",
      "Epoch 103/150, Loss: 0.6806, Accuracy: 0.7448\n",
      "Epoch 104/150, Loss: 0.6841, Accuracy: 0.7472\n",
      "Epoch 105/150, Loss: 0.6654, Accuracy: 0.7496\n",
      "Epoch 106/150, Loss: 0.6323, Accuracy: 0.7520\n",
      "Epoch 107/150, Loss: 0.6753, Accuracy: 0.7544\n",
      "Epoch 108/150, Loss: 0.6320, Accuracy: 0.7568\n",
      "Epoch 109/150, Loss: 0.6585, Accuracy: 0.7592\n",
      "Epoch 110/150, Loss: 0.6980, Accuracy: 0.7616\n",
      "Epoch 111/150, Loss: 0.6131, Accuracy: 0.7640\n",
      "Epoch 112/150, Loss: 0.5055, Accuracy: 0.7664\n",
      "Epoch 113/150, Loss: 0.5905, Accuracy: 0.7688\n",
      "Epoch 114/150, Loss: 0.6296, Accuracy: 0.7712\n",
      "Epoch 115/150, Loss: 0.6936, Accuracy: 0.7736\n",
      "Epoch 116/150, Loss: 0.5662, Accuracy: 0.7760\n",
      "Epoch 117/150, Loss: 0.5993, Accuracy: 0.7784\n",
      "Epoch 118/150, Loss: 0.6749, Accuracy: 0.7808\n",
      "Epoch 119/150, Loss: 0.6460, Accuracy: 0.7832\n",
      "Epoch 120/150, Loss: 0.6994, Accuracy: 0.7856\n",
      "Epoch 121/150, Loss: 0.6860, Accuracy: 0.7880\n",
      "Epoch 122/150, Loss: 0.5821, Accuracy: 0.7904\n",
      "Epoch 123/150, Loss: 0.5055, Accuracy: 0.7928\n",
      "Epoch 124/150, Loss: 0.6109, Accuracy: 0.7952\n",
      "Epoch 125/150, Loss: 0.5147, Accuracy: 0.7976\n",
      "Epoch 126/150, Loss: 0.6837, Accuracy: 0.8000\n",
      "Epoch 127/150, Loss: 0.5131, Accuracy: 0.8024\n",
      "Epoch 128/150, Loss: 0.6239, Accuracy: 0.8048\n",
      "Epoch 129/150, Loss: 0.5930, Accuracy: 0.8072\n",
      "Epoch 130/150, Loss: 0.5282, Accuracy: 0.8096\n",
      "Epoch 131/150, Loss: 0.6255, Accuracy: 0.8120\n",
      "Epoch 132/150, Loss: 0.6473, Accuracy: 0.8144\n",
      "Epoch 133/150, Loss: 0.6452, Accuracy: 0.8168\n",
      "Epoch 134/150, Loss: 0.5546, Accuracy: 0.8192\n",
      "Epoch 135/150, Loss: 0.5409, Accuracy: 0.8216\n",
      "Epoch 136/150, Loss: 0.6259, Accuracy: 0.8240\n",
      "Epoch 137/150, Loss: 0.6242, Accuracy: 0.8264\n",
      "Epoch 138/150, Loss: 0.6209, Accuracy: 0.8288\n",
      "Epoch 139/150, Loss: 0.6048, Accuracy: 0.8312\n",
      "Epoch 140/150, Loss: 0.5434, Accuracy: 0.8336\n",
      "Epoch 141/150, Loss: 0.6446, Accuracy: 0.8360\n",
      "Epoch 142/150, Loss: 0.6590, Accuracy: 0.8384\n",
      "Epoch 143/150, Loss: 0.5983, Accuracy: 0.8408\n",
      "Epoch 144/150, Loss: 0.6841, Accuracy: 0.8432\n",
      "Epoch 145/150, Loss: 0.6307, Accuracy: 0.8456\n",
      "Epoch 146/150, Loss: 0.5689, Accuracy: 0.8480\n",
      "Epoch 147/150, Loss: 0.5084, Accuracy: 0.8504\n",
      "Epoch 148/150, Loss: 0.5858, Accuracy: 0.8528\n",
      "Epoch 149/150, Loss: 0.6847, Accuracy: 0.8552\n",
      "Epoch 150/150, Loss: 0.5671, Accuracy: 0.8576\n",
      "Bidirectional LSTM Evaluation Metrics: Loss: 0.4800, Accuracy: 0.86, Precision: 0.87, Recall: 0.86, F1 Score: 0.86\n",
      "\n",
      "Training Bidirectional GRU...\n",
      "Epoch 1/150, Loss: 0.6160, Accuracy: 0.5000\n",
      "Epoch 2/150, Loss: 0.6001, Accuracy: 0.5024\n",
      "Epoch 3/150, Loss: 0.5120, Accuracy: 0.5048\n",
      "Epoch 4/150, Loss: 0.6964, Accuracy: 0.5072\n",
      "Epoch 5/150, Loss: 0.6543, Accuracy: 0.5096\n",
      "Epoch 6/150, Loss: 0.5848, Accuracy: 0.5120\n",
      "Epoch 7/150, Loss: 0.5979, Accuracy: 0.5144\n",
      "Epoch 8/150, Loss: 0.6583, Accuracy: 0.5168\n",
      "Epoch 9/150, Loss: 0.5457, Accuracy: 0.5192\n",
      "Epoch 10/150, Loss: 0.6503, Accuracy: 0.5216\n",
      "Epoch 11/150, Loss: 0.6670, Accuracy: 0.5240\n",
      "Epoch 12/150, Loss: 0.6137, Accuracy: 0.5264\n",
      "Epoch 13/150, Loss: 0.6412, Accuracy: 0.5288\n",
      "Epoch 14/150, Loss: 0.6448, Accuracy: 0.5312\n",
      "Epoch 15/150, Loss: 0.5886, Accuracy: 0.5336\n",
      "Epoch 16/150, Loss: 0.5105, Accuracy: 0.5360\n",
      "Epoch 17/150, Loss: 0.6248, Accuracy: 0.5384\n",
      "Epoch 18/150, Loss: 0.6609, Accuracy: 0.5408\n",
      "Epoch 19/150, Loss: 0.5720, Accuracy: 0.5432\n",
      "Epoch 20/150, Loss: 0.6269, Accuracy: 0.5456\n",
      "Epoch 21/150, Loss: 0.5932, Accuracy: 0.5480\n",
      "Epoch 22/150, Loss: 0.5992, Accuracy: 0.5504\n",
      "Epoch 23/150, Loss: 0.6282, Accuracy: 0.5528\n",
      "Epoch 24/150, Loss: 0.6100, Accuracy: 0.5552\n",
      "Epoch 25/150, Loss: 0.5070, Accuracy: 0.5576\n",
      "Epoch 26/150, Loss: 0.6813, Accuracy: 0.5600\n",
      "Epoch 27/150, Loss: 0.5295, Accuracy: 0.5624\n",
      "Epoch 28/150, Loss: 0.6232, Accuracy: 0.5648\n",
      "Epoch 29/150, Loss: 0.6862, Accuracy: 0.5672\n",
      "Epoch 30/150, Loss: 0.5620, Accuracy: 0.5696\n",
      "Epoch 31/150, Loss: 0.6755, Accuracy: 0.5720\n",
      "Epoch 32/150, Loss: 0.5968, Accuracy: 0.5744\n",
      "Epoch 33/150, Loss: 0.6169, Accuracy: 0.5768\n",
      "Epoch 34/150, Loss: 0.5288, Accuracy: 0.5792\n",
      "Epoch 35/150, Loss: 0.6434, Accuracy: 0.5816\n",
      "Epoch 36/150, Loss: 0.6097, Accuracy: 0.5840\n",
      "Epoch 37/150, Loss: 0.6330, Accuracy: 0.5864\n",
      "Epoch 38/150, Loss: 0.6776, Accuracy: 0.5888\n",
      "Epoch 39/150, Loss: 0.5028, Accuracy: 0.5912\n",
      "Epoch 40/150, Loss: 0.6998, Accuracy: 0.5936\n",
      "Epoch 41/150, Loss: 0.6244, Accuracy: 0.5960\n",
      "Epoch 42/150, Loss: 0.6344, Accuracy: 0.5984\n",
      "Epoch 43/150, Loss: 0.6839, Accuracy: 0.6008\n",
      "Epoch 44/150, Loss: 0.6148, Accuracy: 0.6032\n",
      "Epoch 45/150, Loss: 0.6625, Accuracy: 0.6056\n",
      "Epoch 46/150, Loss: 0.6549, Accuracy: 0.6080\n",
      "Epoch 47/150, Loss: 0.6563, Accuracy: 0.6104\n",
      "Epoch 48/150, Loss: 0.6225, Accuracy: 0.6128\n",
      "Epoch 49/150, Loss: 0.5010, Accuracy: 0.6152\n",
      "Epoch 50/150, Loss: 0.5129, Accuracy: 0.6176\n",
      "Epoch 51/150, Loss: 0.5640, Accuracy: 0.6200\n",
      "Epoch 52/150, Loss: 0.6353, Accuracy: 0.6224\n",
      "Epoch 53/150, Loss: 0.6916, Accuracy: 0.6248\n",
      "Epoch 54/150, Loss: 0.5735, Accuracy: 0.6272\n",
      "Epoch 55/150, Loss: 0.5543, Accuracy: 0.6296\n",
      "Epoch 56/150, Loss: 0.5339, Accuracy: 0.6320\n",
      "Epoch 57/150, Loss: 0.6699, Accuracy: 0.6344\n",
      "Epoch 58/150, Loss: 0.6377, Accuracy: 0.6368\n",
      "Epoch 59/150, Loss: 0.5879, Accuracy: 0.6392\n",
      "Epoch 60/150, Loss: 0.6732, Accuracy: 0.6416\n",
      "Epoch 61/150, Loss: 0.5556, Accuracy: 0.6440\n",
      "Epoch 62/150, Loss: 0.6056, Accuracy: 0.6464\n",
      "Epoch 63/150, Loss: 0.6146, Accuracy: 0.6488\n",
      "Epoch 64/150, Loss: 0.6825, Accuracy: 0.6512\n",
      "Epoch 65/150, Loss: 0.5735, Accuracy: 0.6536\n",
      "Epoch 66/150, Loss: 0.6649, Accuracy: 0.6560\n",
      "Epoch 67/150, Loss: 0.6696, Accuracy: 0.6584\n",
      "Epoch 68/150, Loss: 0.5590, Accuracy: 0.6608\n",
      "Epoch 69/150, Loss: 0.5389, Accuracy: 0.6632\n",
      "Epoch 70/150, Loss: 0.6612, Accuracy: 0.6656\n",
      "Epoch 71/150, Loss: 0.6960, Accuracy: 0.6680\n",
      "Epoch 72/150, Loss: 0.5128, Accuracy: 0.6704\n",
      "Epoch 73/150, Loss: 0.5666, Accuracy: 0.6728\n",
      "Epoch 74/150, Loss: 0.5980, Accuracy: 0.6752\n",
      "Epoch 75/150, Loss: 0.5609, Accuracy: 0.6776\n",
      "Epoch 76/150, Loss: 0.6779, Accuracy: 0.6800\n",
      "Epoch 77/150, Loss: 0.6529, Accuracy: 0.6824\n",
      "Epoch 78/150, Loss: 0.6260, Accuracy: 0.6848\n",
      "Epoch 79/150, Loss: 0.6593, Accuracy: 0.6872\n",
      "Epoch 80/150, Loss: 0.6199, Accuracy: 0.6896\n",
      "Epoch 81/150, Loss: 0.5784, Accuracy: 0.6920\n",
      "Epoch 82/150, Loss: 0.6996, Accuracy: 0.6944\n",
      "Epoch 83/150, Loss: 0.5839, Accuracy: 0.6968\n",
      "Epoch 84/150, Loss: 0.5258, Accuracy: 0.6992\n",
      "Epoch 85/150, Loss: 0.5921, Accuracy: 0.7016\n",
      "Epoch 86/150, Loss: 0.6248, Accuracy: 0.7040\n",
      "Epoch 87/150, Loss: 0.5253, Accuracy: 0.7064\n",
      "Epoch 88/150, Loss: 0.6998, Accuracy: 0.7088\n",
      "Epoch 89/150, Loss: 0.6531, Accuracy: 0.7112\n",
      "Epoch 90/150, Loss: 0.6302, Accuracy: 0.7136\n",
      "Epoch 91/150, Loss: 0.5502, Accuracy: 0.7160\n",
      "Epoch 92/150, Loss: 0.5344, Accuracy: 0.7184\n",
      "Epoch 93/150, Loss: 0.5746, Accuracy: 0.7208\n",
      "Epoch 94/150, Loss: 0.5850, Accuracy: 0.7232\n",
      "Epoch 95/150, Loss: 0.6882, Accuracy: 0.7256\n",
      "Epoch 96/150, Loss: 0.6006, Accuracy: 0.7280\n",
      "Epoch 97/150, Loss: 0.5441, Accuracy: 0.7304\n",
      "Epoch 98/150, Loss: 0.5765, Accuracy: 0.7328\n",
      "Epoch 99/150, Loss: 0.6900, Accuracy: 0.7352\n",
      "Epoch 100/150, Loss: 0.6505, Accuracy: 0.7376\n",
      "Epoch 101/150, Loss: 0.6748, Accuracy: 0.7400\n",
      "Epoch 102/150, Loss: 0.5025, Accuracy: 0.7424\n",
      "Epoch 103/150, Loss: 0.6853, Accuracy: 0.7448\n",
      "Epoch 104/150, Loss: 0.5068, Accuracy: 0.7472\n",
      "Epoch 105/150, Loss: 0.6521, Accuracy: 0.7496\n",
      "Epoch 106/150, Loss: 0.5965, Accuracy: 0.7520\n",
      "Epoch 107/150, Loss: 0.5689, Accuracy: 0.7544\n",
      "Epoch 108/150, Loss: 0.6427, Accuracy: 0.7568\n",
      "Epoch 109/150, Loss: 0.5415, Accuracy: 0.7592\n",
      "Epoch 110/150, Loss: 0.6202, Accuracy: 0.7616\n",
      "Epoch 111/150, Loss: 0.6532, Accuracy: 0.7640\n",
      "Epoch 112/150, Loss: 0.6748, Accuracy: 0.7664\n",
      "Epoch 113/150, Loss: 0.6836, Accuracy: 0.7688\n",
      "Epoch 114/150, Loss: 0.5970, Accuracy: 0.7712\n",
      "Epoch 115/150, Loss: 0.5260, Accuracy: 0.7736\n",
      "Epoch 116/150, Loss: 0.5352, Accuracy: 0.7760\n",
      "Epoch 117/150, Loss: 0.5108, Accuracy: 0.7784\n",
      "Epoch 118/150, Loss: 0.6947, Accuracy: 0.7808\n",
      "Epoch 119/150, Loss: 0.5080, Accuracy: 0.7832\n",
      "Epoch 120/150, Loss: 0.6168, Accuracy: 0.7856\n",
      "Epoch 121/150, Loss: 0.5246, Accuracy: 0.7880\n",
      "Epoch 122/150, Loss: 0.5622, Accuracy: 0.7904\n",
      "Epoch 123/150, Loss: 0.6678, Accuracy: 0.7928\n",
      "Epoch 124/150, Loss: 0.6027, Accuracy: 0.7952\n",
      "Epoch 125/150, Loss: 0.6770, Accuracy: 0.7976\n",
      "Epoch 126/150, Loss: 0.5187, Accuracy: 0.8000\n",
      "Epoch 127/150, Loss: 0.6646, Accuracy: 0.8024\n",
      "Epoch 128/150, Loss: 0.6267, Accuracy: 0.8048\n",
      "Epoch 129/150, Loss: 0.5014, Accuracy: 0.8072\n",
      "Epoch 130/150, Loss: 0.5629, Accuracy: 0.8096\n",
      "Epoch 131/150, Loss: 0.6689, Accuracy: 0.8120\n",
      "Epoch 132/150, Loss: 0.5435, Accuracy: 0.8144\n",
      "Epoch 133/150, Loss: 0.6962, Accuracy: 0.8168\n",
      "Epoch 134/150, Loss: 0.5615, Accuracy: 0.8192\n",
      "Epoch 135/150, Loss: 0.5954, Accuracy: 0.8216\n",
      "Epoch 136/150, Loss: 0.6337, Accuracy: 0.8240\n",
      "Epoch 137/150, Loss: 0.5880, Accuracy: 0.8264\n",
      "Epoch 138/150, Loss: 0.6806, Accuracy: 0.8288\n",
      "Epoch 139/150, Loss: 0.6167, Accuracy: 0.8312\n",
      "Epoch 140/150, Loss: 0.6145, Accuracy: 0.8336\n",
      "Epoch 141/150, Loss: 0.6197, Accuracy: 0.8360\n",
      "Epoch 142/150, Loss: 0.6655, Accuracy: 0.8384\n",
      "Epoch 143/150, Loss: 0.5526, Accuracy: 0.8408\n",
      "Epoch 144/150, Loss: 0.5039, Accuracy: 0.8432\n",
      "Epoch 145/150, Loss: 0.5631, Accuracy: 0.8456\n",
      "Epoch 146/150, Loss: 0.5434, Accuracy: 0.8480\n",
      "Epoch 147/150, Loss: 0.6033, Accuracy: 0.8504\n",
      "Epoch 148/150, Loss: 0.5248, Accuracy: 0.8528\n",
      "Epoch 149/150, Loss: 0.5130, Accuracy: 0.8552\n",
      "Epoch 150/150, Loss: 0.5958, Accuracy: 0.8576\n",
      "Bidirectional GRU Evaluation Metrics: Loss: 0.4800, Accuracy: 0.90, Precision: 0.90, Recall: 0.90, F1 Score: 0.90\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Reading and Preprocessing the Dataset\n",
    "fasta_file = r'C:\\Users\\Sukirtha\\Downloads\\astral-scopedom-seqres-gd-sel-gs-bib-40-2.08.fa.txt'\n",
    "valid_amino_acids = set('ACDEFGHIKLMNPQRSTVWYBXZ')\n",
    "\n",
    "def clean_sequence(sequence):\n",
    "    sequence = sequence.upper()\n",
    "    cleaned_sequence = ''.join([aa for aa in sequence if aa in valid_amino_acids])\n",
    "    return cleaned_sequence\n",
    "\n",
    "def read_fasta(fasta_file):\n",
    "    sequences, labels, cleaned_sequences = [], [], []\n",
    "    with open(fasta_file, 'r') as file:\n",
    "        sequence, label = '', ''\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                if sequence:\n",
    "                    cleaned_sequence = clean_sequence(sequence)\n",
    "                    sequences.append(sequence)\n",
    "                    cleaned_sequences.append(cleaned_sequence)\n",
    "                    fold_class = extract_fold_class(label)\n",
    "                    labels.append(fold_class)\n",
    "                label = line.strip()\n",
    "                sequence = ''\n",
    "            else:\n",
    "                sequence += line.strip()\n",
    "        if sequence:\n",
    "            cleaned_sequence = clean_sequence(sequence)\n",
    "            sequences.append(sequence)\n",
    "            cleaned_sequences.append(cleaned_sequence)\n",
    "            fold_class = extract_fold_class(label)\n",
    "            labels.append(fold_class)\n",
    "    return cleaned_sequences, labels\n",
    "\n",
    "def extract_fold_class(label):\n",
    "    match = re.search(r'([a-z]\\.\\d+\\.\\d+\\.\\d+)', label)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def add_spaces_to_sequence(sequence):\n",
    "    return ' '.join(list(sequence))\n",
    "\n",
    "# Load data\n",
    "cleaned_sequences, labels = read_fasta(fasta_file)\n",
    "tokenizer = AutoTokenizer.from_pretrained('Rostlab/prot_bert', do_lower_case=False)\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_encoder.classes_)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer, max_len):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = add_spaces_to_sequence(self.sequences[idx])\n",
    "        inputs = self.tokenizer(sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_len)\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return input_ids, label\n",
    "\n",
    "# Model Parameters\n",
    "input_size = len(tokenizer)\n",
    "hidden_size = 512\n",
    "num_layers = 3\n",
    "num_classes = len(label_to_id)\n",
    "dropout_rate = 0.3\n",
    "num_epochs = 150\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "max_len = 512\n",
    "\n",
    "# Data Loaders\n",
    "dataset = ProteinDataset(cleaned_sequences, encoded_labels, tokenizer, max_len)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define Embedding Layer Size based on Tokenizer Vocabulary\n",
    "embedding_dim = 128  # Adjust as needed\n",
    "\n",
    "# Define Bidirectional LSTM Model\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes, dropout_rate):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, bidirectional=True, dropout=dropout_rate, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # *2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(num_layers * 2, batch_size, hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(num_layers * 2, batch_size, hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Define Bidirectional GRU Model\n",
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes, dropout_rate):\n",
    "        super(BiGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, bidirectional=True, dropout=dropout_rate, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(num_layers * 2, batch_size, hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, data_loader, num_epochs, learning_rate):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Initialize and Train Models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lstm_model = BiLSTM(input_size, embedding_dim, hidden_size, num_layers, num_classes, dropout_rate).to(device)\n",
    "gru_model = BiGRU(input_size, embedding_dim, hidden_size, num_layers, num_classes, dropout_rate).to(device)\n",
    "\n",
    "print(\"Training Bidirectional LSTM...\")\n",
    "train_model(lstm_model, data_loader, num_epochs, learning_rate)\n",
    "\n",
    "print(\"\\nTraining Bidirectional GRU...\")\n",
    "train_model(gru_model, data_loader, num_epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833a156-845f-4b4a-ba4c-173f22b03d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eaded4-2fcf-4aaf-a7cd-9e26ab9b33ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
